# ------------------------------------------------------------
# Streamlit in Snowflake: Cortex Search RAG ãƒãƒ£ãƒƒãƒˆUIç‰ˆ
#  - Cortex Agentä¸ä½¿ç”¨ï¼ˆã‚³ã‚¹ãƒˆæœ€é©åŒ–ç‰ˆï¼‰
#  - Cortex Search + Cortex Complete ã‚’ç›´æ¥ä½¿ç”¨
#  - ãƒãƒ£ãƒƒãƒˆãƒãƒ–ãƒ«ï¼ˆst.chat_message / st.chat_inputï¼‰
#  - å±¥æ­´ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆç›´è¿‘kå¾€å¾©ã‚’æ–‡è„ˆã¸ï¼‰
#  - å‚ç…§PDF/URL/ãƒãƒ£ãƒ³ã‚¯ã®å¯è¦–åŒ–
# ------------------------------------------------------------
# Based on original code by Sakuragi (Snowflake)
# Modified for Sustainability Report Analysis
# ------------------------------------------------------------

from typing import List, Dict, Any, Optional
from datetime import datetime
import time
import streamlit as st
from snowflake.snowpark.context import get_active_session
from snowflake.core import Root

# Cortex Complete ã‚’SQLçµŒç”±ã§å‘¼ã³å‡ºã™é–¢æ•°
def cortex_complete(session, model: str, prompt: str) -> str:
    """Cortex Completeã‚’SQLçµŒç”±ã§å®Ÿè¡Œï¼ˆSiSäº’æ›ï¼‰"""
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    escaped_prompt = prompt.replace("'", "''")
    
    sql = f"""
        SELECT SNOWFLAKE.CORTEX.COMPLETE(
            '{model}',
            '{escaped_prompt}'
        ) AS response
    """
    
    result = session.sql(sql).collect()
    return result[0]['RESPONSE'] if result else ""

# =====================================================
# è¨­å®š
# =====================================================
DEFAULT_DATABASE = "DEMO_DB"
DEFAULT_SCHEMA = "DEMO_SUSTAINABILITY"

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«
MODELS = [
    "claude-4-sonnet",
    "claude-3-7-sonnet",
    "claude-3-5-sonnet",
    "llama4-maverick",
    "llama4-scout",
]

# Cortex Search Servicesï¼ˆå›ºå®šãƒªã‚¹ãƒˆ - å‹•çš„å–å¾—ã‚‚å¯èƒ½ï¼‰
SEARCH_SERVICES = [
    {
        "name": "ã‚¹ãƒãƒ¥ãƒ¯ãƒ¼ãƒ‰ã‚·ãƒƒãƒ—è©•ä¾¡ç”¨",
        "fq_name": "DEMO_DB.DEMO_SUSTAINABILITY.SUSTAINABILITY_REPORT",
        "db": "DEMO_DB",
        "schema": "DEMO_SUSTAINABILITY",
        "short_name": "SUSTAINABILITY_REPORT",
        "search_column": "chunk_text",
        "columns": ["chunk_text", "file_name", "relative_path", "scoped_file_url", "page_index"],
    },
    {
        "name": "ã‚°ãƒ­ãƒ¼ãƒãƒ«å¹´é‡‘åˆ†æç”¨",
        "fq_name": "DEMO_DB.DEMO_SUSTAINABILITY.GLOBAL_PF_SUSTAINABILITY_REPORT",
        "db": "DEMO_DB",
        "schema": "DEMO_SUSTAINABILITY",
        "short_name": "GLOBAL_PF_SUSTAINABILITY_REPORT",
        "search_column": "chunk_text",
        "columns": ["chunk_text", "file_name", "relative_path", "scoped_file_url", "page_index", "source_report"],
    },
]

# Snowflakeæ¥ç¶š
session = get_active_session()
root = Root(session)


# =====================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# =====================================================

def get_cortex_search_service(service_config: Dict[str, Any]):
    """Cortex Search Serviceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—"""
    return root.databases[service_config["db"]].schemas[service_config["schema"]].cortex_search_services[service_config["short_name"]]


def query_cortex_search(
    query: str,
    service_config: Dict[str, Any],
    num_results: int = 5,
    filter_obj: Optional[Dict[str, Any]] = None
) -> tuple[str, List[Dict[str, Any]]]:
    """Cortex Searchã‚’å®Ÿè¡Œã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
    
    svc = get_cortex_search_service(service_config)
    search_col = service_config.get("search_column", "chunk_text")
    request_columns = service_config.get("columns", ["chunk_text", "file_name", "relative_path"])
    
    # æ¤œç´¢å®Ÿè¡Œ
    kwargs = {
        "query": query,
        "columns": request_columns,
        "limit": num_results,
    }
    if filter_obj:
        kwargs["filter"] = filter_obj
    
    doc = svc.search(**kwargs)
    results = doc.results
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
    context_rows = []
    context_lines = []
    
    for i, r in enumerate(results, start=1):
        content = r.get(search_col) or r.get(search_col.lower()) or r.get(search_col.upper()) or ""
        file_name = r.get("file_name") or r.get("FILE_NAME") or ""
        relative_path = r.get("relative_path") or r.get("RELATIVE_PATH") or ""
        file_url = r.get("scoped_file_url") or r.get("SCOPED_FILE_URL") or r.get("file_url") or ""
        page_index = r.get("page_index") or r.get("PAGE_INDEX") or ""
        
        context_rows.append({
            "idx": i,
            "file_name": file_name,
            "relative_path": relative_path,
            "file_url": file_url,
            "page_index": page_index,
            "chunk": content,
        })
        
        # LLMã«æ¸¡ã™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        source_info = f"[ãƒ•ã‚¡ã‚¤ãƒ«: {file_name}"
        if page_index:
            source_info += f", ãƒšãƒ¼ã‚¸: {page_index}"
        source_info += "]"
        context_lines.append(f"--- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {i} {source_info} ---\n{content}\n")
    
    context_text = "\n".join(context_lines)
    return context_text, context_rows


def build_history_text(chat_history: List[Dict[str, Any]], k: int) -> str:
    """éå»ã®ä¼šè©±å±¥æ­´ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
    if k <= 0 or not chat_history:
        return ""
    turns = chat_history[-k:]
    messages = []
    for t in turns:
        messages.append(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {t.get('question', '')}")
        messages.append(f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {t.get('answer', '')}")
    return "\n".join(messages)


def build_prompt(history_text: str, context_text: str, user_query: str, service_name: str) -> str:
    """LLMã«é€ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
    
    system = f"""ã‚ãªãŸã¯{service_name}ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æ¤œç´¢çµæœï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¨ä¼šè©±å±¥æ­´ã®ã¿ã‚’æ ¹æ‹ ã«ã€æ­£ç¢ºã‹ã¤ç°¡æ½”ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€å›ç­”ãƒ«ãƒ¼ãƒ«ã€‘
1. å¿…ãšå‡ºå…¸ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã€ãƒšãƒ¼ã‚¸ç•ªå·ï¼‰ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¨˜è¼‰ãŒãªã„æƒ…å ±ã¯ã€Œè³‡æ–™ã‹ã‚‰ã¯ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸã€ã¨å›ç­”ã—ã¦ãã ã•ã„
3. æ¨æ¸¬ã‚„æ†¶æ¸¬ã¯é¿ã‘ã€äº‹å®Ÿã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„
4. è¤‡æ•°ã®æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ã€ç®‡æ¡æ›¸ãã§æ•´ç†ã—ã¦ãã ã•ã„"""

    parts = [
        f"ã€ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºã€‘\n{system}",
        f"ã€éå»ã®ä¼šè©±ã€‘\n{history_text or '(ãªã—)'}",
        f"ã€æ¤œç´¢çµæœï¼ˆå‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰ã€‘\n{context_text or '(è©²å½“ãªã—)'}",
        f"ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘\n{user_query}",
    ]
    return "\n\n".join(parts)


def stream_text(container, full_text: str, step: int = 80):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ®µéšçš„ã«è¡¨ç¤ºï¼ˆç–‘ä¼¼ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰"""
    buf = ""
    for i in range(0, len(full_text), step):
        buf += full_text[i:i+step]
        container.markdown(buf)
        time.sleep(0.02)


# =====================================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼
# =====================================================

def init_sidebar():
    st.sidebar.header("âš™ï¸ è¨­å®š")
    
    # --- Cortex Search Serviceé¸æŠ ---
    st.sidebar.subheader("æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹")
    
    service_options = {s["name"]: s for s in SEARCH_SERVICES}
    selected_name = st.sidebar.selectbox(
        "Cortex Search Service",
        options=list(service_options.keys()),
        index=0,
    )
    st.session_state.selected_service = service_options[selected_name]
    st.sidebar.caption(f"ğŸ“ {st.session_state.selected_service['fq_name']}")
    
    st.sidebar.divider()
    
    # --- ãƒ¢ãƒ‡ãƒ«é¸æŠ ---
    st.sidebar.subheader("LLMãƒ¢ãƒ‡ãƒ«")
    
    if "selected_model" not in st.session_state:
        st.session_state.selected_model = MODELS[0]
    
    st.session_state.selected_model = st.sidebar.selectbox(
        "å›ç­”ç”Ÿæˆãƒ¢ãƒ‡ãƒ«",
        MODELS,
        index=MODELS.index(st.session_state.selected_model),
    )
    
    st.sidebar.divider()
    
    # --- æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ---
    st.sidebar.subheader("æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    
    if "num_retrieved_chunks" not in st.session_state:
        st.session_state.num_retrieved_chunks = 5
    
    st.session_state.num_retrieved_chunks = st.sidebar.slider(
        "å‚ç…§ãƒãƒ£ãƒ³ã‚¯æ•°",
        min_value=1,
        max_value=15,
        value=st.session_state.num_retrieved_chunks,
        help="æ¤œç´¢ã§å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã®æ•°"
    )
    
    if "history_k" not in st.session_state:
        st.session_state.history_k = 3
    
    st.session_state.history_k = st.sidebar.slider(
        "éå»å±¥æ­´ã®å‚ç…§æ•°",
        min_value=0,
        max_value=10,
        value=st.session_state.history_k,
        help="LLMã«æ¸¡ã™éå»ã®ä¼šè©±ã‚¿ãƒ¼ãƒ³æ•°"
    )
    
    st.sidebar.divider()
    
    # --- ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ ---
    st.sidebar.subheader("ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
    
    if "filter_enabled" not in st.session_state:
        st.session_state.filter_enabled = False
    
    st.session_state.filter_enabled = st.sidebar.toggle(
        "ãƒ•ã‚¡ã‚¤ãƒ«åãƒ•ã‚£ãƒ«ã‚¿ã‚’ä½¿ç”¨",
        value=st.session_state.filter_enabled,
    )
    
    if st.session_state.filter_enabled:
        if "filter_file_name" not in st.session_state:
            st.session_state.filter_file_name = ""
        
        st.session_state.filter_file_name = st.sidebar.text_input(
            "ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰",
            value=st.session_state.filter_file_name,
            placeholder="ä¾‹: AMOne",
        )
    
    st.sidebar.divider()
    
    # --- å±¥æ­´ç®¡ç† ---
    st.sidebar.subheader("å±¥æ­´ç®¡ç†")
    
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    col1, col2 = st.sidebar.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚¯ãƒªã‚¢", use_container_width=True):
            st.session_state.chat_history = []
            st.rerun()
    
    with col2:
        st.sidebar.caption(f"å±¥æ­´: {len(st.session_state.chat_history)}ä»¶")
    
    # --- æƒ…å ±è¡¨ç¤º ---
    st.sidebar.divider()
    st.sidebar.caption("ğŸ’¡ **Cortex Search + Complete**")
    st.sidebar.caption("Cortex Agentã‚’ä½¿ç”¨ã—ãªã„ã‚³ã‚¹ãƒˆæœ€é©åŒ–ç‰ˆã§ã™ã€‚")


# =====================================================
# ãƒãƒ£ãƒƒãƒˆè¡¨ç¤º
# =====================================================

def render_context_expander(context_rows: List[Dict[str, Any]]):
    """å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ã§è¡¨ç¤º"""
    if not context_rows:
        return
    
    with st.expander(f"ğŸ“š å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ({len(context_rows)}ä»¶)", expanded=False):
        for r in context_rows:
            st.markdown(f"**#{r['idx']} - {r['file_name']}**")
            if r.get("page_index"):
                st.caption(f"ğŸ“„ ãƒšãƒ¼ã‚¸: {r['page_index']}")
            
            # ãƒãƒ£ãƒ³ã‚¯å†…å®¹ï¼ˆæŠ˜ã‚ŠãŸãŸã¿ï¼‰
            with st.container():
                chunk_preview = r["chunk"][:300] + "..." if len(r["chunk"]) > 300 else r["chunk"]
                st.text(chunk_preview)
            
            if r.get("file_url"):
                st.markdown(f"[ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã]({r['file_url']})")
            
            st.divider()


def render_chat_history():
    """éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º"""
    for turn in st.session_state.get("chat_history", []):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        with st.chat_message("user"):
            st.markdown(turn.get("question", ""))
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        with st.chat_message("assistant"):
            st.markdown(turn.get("answer", ""))
            
            # å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            ctx = turn.get("contexts") or []
            if ctx:
                render_context_expander(ctx)


# =====================================================
# ãƒ¡ã‚¤ãƒ³
# =====================================================

def main():
    st.set_page_config(
        page_title="Cortex Search RAG",
        page_icon="ğŸ”",
        layout="wide",
    )
    
    st.title("ğŸ” Cortex Search RAG ãƒãƒ£ãƒƒãƒˆ")
    st.caption("Cortex Search + Cortex Complete ã«ã‚ˆã‚‹ã‚³ã‚¹ãƒˆæœ€é©åŒ–RAG")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼åˆæœŸåŒ–
    init_sidebar()
    
    service = st.session_state.get("selected_service")
    if not service:
        st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        return
    
    # ç¾åœ¨ã®è¨­å®šã‚’è¡¨ç¤º
    with st.expander("ç¾åœ¨ã®è¨­å®š", expanded=False):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹", service["name"])
        with col2:
            st.metric("LLMãƒ¢ãƒ‡ãƒ«", st.session_state.selected_model)
        with col3:
            st.metric("å‚ç…§ãƒãƒ£ãƒ³ã‚¯æ•°", st.session_state.num_retrieved_chunks)
    
    st.divider()
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º
    render_chat_history()
    
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    user_query = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")
    
    if user_query:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å³åº§ã«è¡¨ç¤º
        with st.chat_message("user"):
            st.markdown(user_query)
        
        # ãƒ•ã‚£ãƒ«ã‚¿æ§‹ç¯‰
        filter_obj = None
        if st.session_state.get("filter_enabled") and st.session_state.get("filter_file_name"):
            # æ³¨æ„: Cortex Searchã®ãƒ•ã‚£ãƒ«ã‚¿ã¯ATTRIBUTESåˆ—ã«å¯¾ã—ã¦ã®ã¿æœ‰åŠ¹
            # @contains ã¯ARRAYç”¨ã€ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ä¸€è‡´ã¯æ¤œç´¢ã‚¯ã‚¨ãƒªã«å«ã‚ã‚‹æ–¹ãŒåŠ¹æœçš„
            pass
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”
        with st.chat_message("assistant"):
            with st.spinner("æ¤œç´¢ä¸­..."):
                # 1) Cortex Searchã§æ¤œç´¢
                context_text, context_rows = query_cortex_search(
                    query=user_query,
                    service_config=service,
                    num_results=st.session_state.num_retrieved_chunks,
                    filter_obj=filter_obj,
                )
            
            with st.spinner("å›ç­”ç”Ÿæˆä¸­..."):
                # 2) å±¥æ­´ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
                history_text = build_history_text(
                    st.session_state.get("chat_history", []),
                    st.session_state.history_k
                )
                
                # 3) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
                prompt = build_prompt(
                    history_text=history_text,
                    context_text=context_text,
                    user_query=user_query,
                    service_name=service["name"],
                )
                
                # 4) LLMå‘¼ã³å‡ºã—ï¼ˆSQLçµŒç”±ï¼‰
                placeholder = st.empty()
                try:
                    answer = cortex_complete(
                        session=session,
                        model=st.session_state.selected_model,
                        prompt=prompt
                    )
                except Exception as e:
                    answer = f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                
                # ç–‘ä¼¼ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                stream_text(placeholder, answer)
            
            # å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
            render_context_expander(context_rows)
        
        # å±¥æ­´ã«ä¿å­˜
        turn = {
            "timestamp": datetime.now().isoformat(timespec="seconds"),
            "question": user_query,
            "answer": answer,
            "model": st.session_state.selected_model,
            "contexts": context_rows,
        }
        st.session_state.chat_history.append(turn)


if __name__ == "__main__":
    main()

